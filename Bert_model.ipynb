{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOaI9ZAnqDL8rllNkZPjTU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c299fce1ab714d0eb905a1269f40b7e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_106c7ac040b54222a906dcb58e9d28ad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_10b88b95c11b4922ac72b2de06f56ae4",
              "IPY_MODEL_e842a20f2a8a4283939061512b633c15"
            ]
          }
        },
        "106c7ac040b54222a906dcb58e9d28ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10b88b95c11b4922ac72b2de06f56ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_038818e70a5a41c0819a0075832849b7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_962aa5b20d154be694cf70f7f207c583"
          }
        },
        "e842a20f2a8a4283939061512b633c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3d1444ddd7194056bc9f96d6c75cdf78",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.04MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47c0ea9de978489fabe7dd204002d1f1"
          }
        },
        "038818e70a5a41c0819a0075832849b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "962aa5b20d154be694cf70f7f207c583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d1444ddd7194056bc9f96d6c75cdf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47c0ea9de978489fabe7dd204002d1f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericpapain/DiscriminationTweetsProject/blob/Eric/Bert_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eosrJjJa4pBo",
        "outputId": "f3d9b347-051e-4822-b536-3fc2a8ed8b8b"
      },
      "source": [
        "### Initialisation de Collab dans mon drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_QuyqfP56ER"
      },
      "source": [
        "## **1. Import dataset and librairies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OveyvAgP6Czz"
      },
      "source": [
        "#### *1.1 Import Librairies*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7HQmqIP6LRt",
        "outputId": "92691cab-3e02-4c55-8cd0-f2303cbf1d68"
      },
      "source": [
        "!pip install pyspellchecker"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/96/827c132397d0eb5731c1eda05dbfb019ede064ca8c7d0f329160ce0a4acd/pyspellchecker-0.5.5-py2.py3-none-any.whl (1.9MB)\n",
            "\r\u001b[K     |▏                               | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 31.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 22.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 18.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 13.6MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 14.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 14.0MB/s eta 0:00:01\r\u001b[K     |█▍                              | 81kB 13.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92kB 14.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 102kB 14.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 112kB 14.0MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 14.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 133kB 14.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 143kB 14.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 153kB 14.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 163kB 14.0MB/s eta 0:00:01\r\u001b[K     |███                             | 174kB 14.0MB/s eta 0:00:01\r\u001b[K     |███                             | 184kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 194kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 204kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 215kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 225kB 14.0MB/s eta 0:00:01\r\u001b[K     |████                            | 235kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 245kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 256kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 266kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 276kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 286kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 296kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 307kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 317kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 327kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 337kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 348kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 358kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 368kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 378kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 389kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 399kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 409kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 419kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 430kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 440kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 450kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 460kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 471kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 481kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 491kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 501kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 512kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 522kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 532kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 542kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 552kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 563kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 573kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 583kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 593kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 604kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 614kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 624kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 634kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 645kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 655kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 665kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 675kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 686kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 696kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 706kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 716kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 727kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 737kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 747kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 757kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 768kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 778kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 788kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 798kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 808kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 819kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 829kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 839kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 849kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 860kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 870kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 880kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 890kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 901kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 911kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 921kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 931kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 942kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 952kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 962kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 972kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 983kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 993kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.4MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.4MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.4MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.4MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.4MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.4MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.4MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.4MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.4MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.5MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.5MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.5MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.5MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.5MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.5MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.5MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.6MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.6MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.6MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.6MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.6MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.6MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.6MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.6MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.6MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.7MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.7MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.7MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.7MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.7MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.7MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.7MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.7MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.7MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.8MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.8MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.8MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.8MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.8MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.8MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.8MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.8MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.8MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.8MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.9MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.9MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.9MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.9MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.9MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.9MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.9MB 14.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1Y3ubXL6E_-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from spellchecker import SpellChecker\n",
        "import requests\n",
        "from datetime import datetime"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-g7WecH6UTz"
      },
      "source": [
        "#### *1.2 Import dataset*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZnNYw-E6OqV"
      },
      "source": [
        "dataset = pd.read_csv('discrimination.csv', sep=',')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMgp_Sj26XAN"
      },
      "source": [
        "discriminant_Dataset = dataset.iloc[:,[1,3]]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwCM-ZHx8Y4d"
      },
      "source": [
        "discriminant_Dataset.sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lIwd7e6_Ju6"
      },
      "source": [
        "## **2. Tokenization and Input Formatting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcghjPv1-Wfq"
      },
      "source": [
        "def data_cleaning(dataset):\n",
        "    '''\n",
        "        df : DataFrame \n",
        "    '''\n",
        "    #capture du temps d'exécution du programme\n",
        "    start_time = datetime.now()\n",
        "    for i in range(len(dataset)):\n",
        "        # Mise en minuscule \n",
        "        dataset['Tweets'][i] = dataset['Tweets'][i].lower()\n",
        "        #suppression des carractère spéciaux\n",
        "        dataset['Tweets'][i]=re.sub(r'(&#[0-9]*;)', ' ',dataset['Tweets'][i])\n",
        "        #suppression des annotations, des noms et tag inutile\n",
        "        dataset['Tweets'][i]=re.sub(r'(@.*?)[\\s]', ' ',dataset['Tweets'][i])\n",
        "        #dataset['Tweet'][i]=re.sub(r'(&.*?)[\\s]', ' ',dataset['Tweet'][i])\n",
        "        # Replace '&amp;' par '&'\n",
        "        dataset['Tweets'][i] = re.sub(r'&amp;', '&', dataset['Tweets'][i])\n",
        "        #suppression urls\n",
        "        dataset['Tweets'][i]=re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', ' ',dataset['Tweets'][i])\n",
        "        #suppression des retweets\n",
        "        dataset['Tweets'][i]=re.sub(r'rt', ' ',dataset['Tweets'][i])\n",
        "        # Suppression des espaces de fin de ligne\n",
        "        dataset['Tweets'][i] = re.sub(r'\\s+', ' ', dataset['Tweets'][i]).strip()\n",
        "        #dataset['Tweets'][i]=re.sub(r'#', ' ',dataset['Tweets'][i])\n",
        "        # suppression des carractères spéciaux et de la ponctuation \n",
        "        #dataset['Tweets'][i] = re.sub(r'(ð|ÿ|⇒|¬|œ|¦|€|˜|™|¸|¤|©|¡|…|”|“|‹|š|±|³|iâ|§|„|~|-|æ|«|€|¶|ŧ|←|↓|→|ø|þ|@|ß|ð|đ|ŋ|ħ|ł|µ|ł|»|¢|)', ' ', dataset['Tweets'][i]) \n",
        "        \n",
        "    #capture temps de fin\n",
        "    end_time = datetime.now()\n",
        "    print(\">>>>> le data cleanning à pris :\",(end_time-start_time),\" secondes.\")\n",
        "    return dataset"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG9pCfIx_pWg"
      },
      "source": [
        "### *- Remplace word abreviation and verlang*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8yIHSX0_mOu"
      },
      "source": [
        "#parcours de chaque mot dans un tweet\n",
        "def remplace_word_abbreviation(dataset_clean,file_txt):\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    list_word=[]\n",
        "    new_list_word=[]\n",
        "    dico_word_abrev={}\n",
        "    #chargement du dico des abrev\n",
        "    a_file = open(file_txt)\n",
        "\n",
        "    for line in a_file:\n",
        "        value = line.split(\",\")\n",
        "        #list_word.append(value)\n",
        "    for val in value:\n",
        "        valN = val.strip(\"{}\")\n",
        "        list_word.append(valN)\n",
        "    for val in list_word:\n",
        "        valN = val.split(\":\")\n",
        "        try:\n",
        "            key = valN[0]\n",
        "            val_key = valN[1]\n",
        "            dico_word_abrev[key] = val_key\n",
        "        except:\n",
        "            #print(key, \"------------------------\",val_key)\n",
        "            del val\n",
        "            pass\n",
        "    \n",
        "    for word_dico,values in dico_word_abrev.items():\n",
        "        for i in range(len(dataset_clean)):\n",
        "            tweet = list((dataset_clean['Tweets'][i]).split())\n",
        "            #positionnement au premier mot\n",
        "            j=0\n",
        "            for word in tweet:\n",
        "                #recupération du mot en non abrégé\n",
        "                if(word == word_dico):\n",
        "                    #remplacement dans le tweet\n",
        "                    tweet[j]=values\n",
        "                #tweet suivant\n",
        "                j=j+1\n",
        "            if(j==0):\n",
        "                pass\n",
        "            else:\n",
        "                #mise a jour du tweet dans le dataset\n",
        "                dataset['Tweets'][i] = ' '.join(tweet)\n",
        "            #if(i==len(dataset_clean)-1):\n",
        "                #print(i)\n",
        "    \n",
        "    end_time = datetime.now()\n",
        "    print(\">>>>> le remplacement des mots abégé et ecris en argo à pris :\",(end_time-start_time),\" secondes.\")\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRaAVHit_un4"
      },
      "source": [
        "### *- Spelling word*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Si8WCkj_saM"
      },
      "source": [
        "#recherche de mot mal écrit\n",
        "def spelling_word_in_tweet(dataset_clean):\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    spell = SpellChecker(language='en',distance=1)\n",
        "    dico_correction_orthographe = {}\n",
        "    \n",
        "    for i in range(len(dataset_clean)):\n",
        "        tweet = list((dataset_clean['Tweets'][i]).split())\n",
        "        j=0\n",
        "        for word in tweet:\n",
        "            # Get the one `most likely` answer\n",
        "            #print(\"next : \",word,\" newww :\",spell.correction(word))\n",
        "            mot = spell.correction(word)\n",
        "            # Get a list of `likely` options\n",
        "           # print(spell.candidates(word))\n",
        "            if(mot==word):\n",
        "                pass\n",
        "            else:\n",
        "                #construction du dico des abréviation trouvé\n",
        "                #print(mot,word)\n",
        "                dico_correction_orthographe[word]=mot\n",
        "                tweet[j]=mot\n",
        "                #print(\"tweet number : \",i,\" i was :\",word,\" now i'm \",true_val)\n",
        "            j=j+1\n",
        "            #print(r.text[startIndex:endIndex])\n",
        "        if(j==0):\n",
        "            pass\n",
        "        else:\n",
        "            #mise a jour du tweet\n",
        "            dataset_clean['Tweets'][i] = ' '.join(tweet)\n",
        "            dataset_clean['Tweets'][i] = dataset_clean['Tweets'][i].lower()\n",
        "        #if((i+1)%len(dataset_clean)==0):\n",
        "            #print(i,\": tous les tweets ont été corrigé !!!\")\n",
        "    \n",
        "    end_time = datetime.now()\n",
        "    print(\">>>>> le remplacement des mots mal écris ( fautes d'othographes et de grammaires) à pris :\",(end_time-start_time),\" secondes.\")\n",
        "    \n",
        "    return dico_correction_orthographe, dataset"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEe40q8G_x6R"
      },
      "source": [
        "def traitemnet_before_bert_tokenization(dataset):\n",
        "    #import file abrev \n",
        "    file = \"dico_abrev.txt\"\n",
        "    # small cleaning\n",
        "    dataset_clean=data_cleaning(dataset)\n",
        "    # abreviation subtitution\n",
        "    dataset_with_abrev_ok=remplace_word_abbreviation(dataset_clean,file)\n",
        "    # manage spelling word\n",
        "    _, dataset_spell_ok=spelling_word_in_tweet(dataset_with_abrev_ok)\n",
        "    \n",
        "    return dataset_spell_ok"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9JfDXTM_0YF",
        "outputId": "8f808652-8fb6-456c-86f7-93b8204d5c76"
      },
      "source": [
        "#small traitemnt\n",
        "discriminant_Dataset = traitemnet_before_bert_tokenization(discriminant_Dataset)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>>> le remplacement des mots abégé et ecris en argo à pris : 0:48:57.432129  secondes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>>>> le remplacement des mots mal écris ( fautes d'othographes et de grammaires) à pris : 0:00:55.512227  secondes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gwsCIJJ6cWj"
      },
      "source": [
        "# divided our dataset in test and train\n",
        "discriminant_Dataset['split']= np.random.randn(discriminant_Dataset.shape[0], 1)\n",
        "msk = np.random.rand(len(discriminant_Dataset)) <= 0.9\n",
        "#data_train and data_test\n",
        "data_train = discriminant_Dataset[msk]\n",
        "data_test = discriminant_Dataset[~msk]\n",
        "\n",
        "data_train = data_train.iloc[:,[1,3]]\n",
        "data_test = data_test.iloc[:,[1,3]]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrTbQlSm_60O"
      },
      "source": [
        "#### *Training/Validation Split*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D703_ETa_9Xm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3cbQqI0AUbW"
      },
      "source": [
        "#for discriminant data\n",
        "X_discri_train, X_discri_val, y_discri_train, y_discri_val = train_test_split(data_train['Tweets'], \n",
        "                                                  data_train['Labels'], \n",
        "                                                  test_size=0.10, \n",
        "                                                  random_state=2020, \n",
        "                                                  stratify=data_train['Labels'])"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqlYjFOVAl73"
      },
      "source": [
        "#### *2.1. BERT Tokenizer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgNTu7KsaCQW",
        "outputId": "6d79f693-3d7c-4dc6-cbec-cb7b9f29ab2e"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 22.5MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 30.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 22.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 20.1MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 21.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 16.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 16.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 17.3MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 15.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 16.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 16.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122kB 16.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133kB 16.4MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 16.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 153kB 16.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 163kB 16.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 174kB 16.4MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 16.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 194kB 16.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 16.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 16.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 245kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 256kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 266kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 296kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 307kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 317kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 327kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 337kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 348kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 358kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 368kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 378kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 389kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 399kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 409kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 419kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 440kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 450kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 460kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 471kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 481kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 491kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 501kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 512kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 522kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 532kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 542kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 552kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 563kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 573kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 583kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 593kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 604kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 614kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 624kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 634kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 645kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 655kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 665kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 675kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 686kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 696kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 706kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 716kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 727kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 737kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 747kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 757kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 768kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 778kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 788kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 798kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 808kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 819kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 829kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 839kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 849kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 860kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 870kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 880kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 890kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 901kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 911kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 921kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 931kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 942kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 952kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 962kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 972kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 983kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 993kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.0MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2MB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4MB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4MB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 16.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5MB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.5MB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 16.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=b53c66f4c9e1577eb26d59280a147cc6b8b95f7878c528c90b6bfc4df4d3e347\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "c299fce1ab714d0eb905a1269f40b7e5",
            "106c7ac040b54222a906dcb58e9d28ad",
            "10b88b95c11b4922ac72b2de06f56ae4",
            "e842a20f2a8a4283939061512b633c15",
            "038818e70a5a41c0819a0075832849b7",
            "962aa5b20d154be694cf70f7f207c583",
            "3d1444ddd7194056bc9f96d6c75cdf78",
            "47c0ea9de978489fabe7dd204002d1f1"
          ]
        },
        "id": "fHuVQXoiAnOh",
        "outputId": "d6c2136a-7d69-44d7-cbc0-64838f659997"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c299fce1ab714d0eb905a1269f40b7e5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-mtKeHEAuG-",
        "outputId": "558b82d2-6e9f-483c-b8a4-04207c76bc02"
      },
      "source": [
        "#extraction of ours tweet\n",
        "all_tweets = dataset.Tweets.values\n",
        "# Encode our dataset in a list for finding a max lenght of our dataset\n",
        "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
        "\n",
        "# Find the maximum length of tweet after encoding\n",
        "MAX_LEN = max([len(sent) for sent in encoded_tweets])\n",
        "print('Max length: ', MAX_LEN)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length:  61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3UHcUeMAu_O"
      },
      "source": [
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data_after_clean):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data_after_clean:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            sent,                           # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,             # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(\">>>>> La Tokenization à pris :\",(end_time-start_time),\" secondes.\")\n",
        "    \n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeICXGFHA1SA"
      },
      "source": [
        "### -------------Tokenize our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhYkVo4PAzAm",
        "outputId": "c9d38e2b-545c-48c4-bd28-a983309b5774"
      },
      "source": [
        "# Run function `preprocessing_for_bert` on the train set and the validation set for discriminant dataset\n",
        "print('Tokenizing data...')\n",
        "train_discr_inputs, train_discr_masks = preprocessing_for_bert(X_discri_train)\n",
        "val_discr_inputs, val_discr_masks = preprocessing_for_bert(X_discri_val)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>>>> La Tokenization à pris : 0:00:07.288274  secondes.\n",
            ">>>>> La Tokenization à pris : 0:00:00.796605  secondes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7NgR7J8BAWN"
      },
      "source": [
        "### **2.2. Create PyTorch DataLoader**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emQgVCfVBCxE"
      },
      "source": [
        "We will create an iterator for our dataset using the torch DataLoader class. This will help save on memory during training and boost the training speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELueY8svBA-O"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZTSl99aBFuH"
      },
      "source": [
        "# Convert other data types to torch.Tensor for discrimination dataset\n",
        "train_discri_labels = torch.tensor(y_discri_train.values)\n",
        "val_discri_labels = torch.tensor(y_discri_val.values)\n",
        "\n",
        "# Create the DataLoader for our training set for discrimination dataset\n",
        "train_discri_data = TensorDataset(train_discr_inputs, train_discr_masks, train_discri_labels)\n",
        "train_d_sampler = RandomSampler(train_discri_data)\n",
        "train_discri_dataloader = DataLoader(train_discri_data, sampler=train_d_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set for discrimination dataset\n",
        "val_discri_data = TensorDataset(val_discr_inputs, val_discr_masks, val_discri_labels)\n",
        "val_d_sampler = SequentialSampler(val_discri_data)\n",
        "val_discri_dataloader = DataLoader(val_discri_data, sampler=val_d_sampler, batch_size=batch_size)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcKj6JQ7BOxA"
      },
      "source": [
        "## **3. Train Our Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d83NdHvNBSl0"
      },
      "source": [
        "BERT-base consists of 12 transformer layers, each transformer layer takes in a list of token embeddings, and produces the same number of embeddings with the same hidden size (or dimensions) on the output. The output of the final transformer layer of the [CLS] token is used as the features of the sequence to feed a classifier.\n",
        "\n",
        "The transformers library has the BertForSequenceClassification class which is designed for classification tasks. However, we will create a new class so we can specify our own choice of classifiers.\n",
        "\n",
        "Below we will create a BertClassifier class with a BERT model to extract the last hidden layer of the [CLS] token and a single-hidden-layer feed-forward neural network as our classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBW6vzlxBWIg"
      },
      "source": [
        "#### *3.1. Create BertClassifier*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6b9ec0rBQ36",
        "outputId": "3c10a151-8076-4104-eaa2-4ef1c35c556f"
      },
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 100, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 34 µs, sys: 7 µs, total: 41 µs\n",
            "Wall time: 45.1 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khkRrG4HBaaM"
      },
      "source": [
        "#### *3.2. Optimizer & Learning Rate Scheduler*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOr-lVARBcYD"
      },
      "source": [
        "To fine-tune our Bert Classifier, we need to create an optimizer. The authors recommend following hyper-parameters:\n",
        "\n",
        "- Batch size: 16 or 32\n",
        "- Learning rate (Adam): 5e-5, 3e-5 or 2e-5\n",
        "- Number of epochs: 2, 3, 4\n",
        "\n",
        "Huggingface provided the run_glue.py script, an examples of implementing the transformers library. In the script, the AdamW optimizer is used.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC1lhp2CBfkt"
      },
      "source": [
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU or cpu\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    #choice of CPU or GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_discri_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrphamqHBkWO"
      },
      "source": [
        "#### *3.3. Training Loop*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA27MZ2DBlOx"
      },
      "source": [
        "We will train our Bert Classifier for 4 epochs. In each epoch, we will train our model and evaluate its performance on the validation set. In more details, we will:\n",
        "\n",
        "Training:\n",
        "- Unpack our data from the dataloader and load the data onto the GPU\n",
        "- Zero out gradients calculated in the previous pass\n",
        "- Perform a forward pass to compute logits and loss\n",
        "- Perform a backward pass to compute gradients (loss.backward())\n",
        "- Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "- Update the model's parameters (optimizer.step())\n",
        "- Update the learning rate (scheduler.step())\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "- Unpack our data and load onto the GPU\n",
        "- Forward pass\n",
        "- Compute loss and accuracy rate over the validation set\n",
        "\n",
        "The script below is commented with the details of our training and evaluation loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jNQ6gNJBoTL"
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    start_time = datetime.now()\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            \n",
        "            # Tell PyTorch to run the model on GPU or cpu\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            # Load batch to GPU or CPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    end_time = datetime.now()\n",
        "    print(\">>>>>Training complete! in \",(end_time-start_time),\" secondes.\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Tell PyTorch to run the model on GPU or cpu\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # Load batch to GPU or CPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liu454_UB3_C"
      },
      "source": [
        "#### **----Now, let's start training our BertClassifier!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h26e7f74B52N",
        "outputId": "b6c0934a-58e8-415e-adb6-6e7461f5759b"
      },
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=4)\n",
        "train(bert_classifier, train_discri_dataloader, val_discri_dataloader, epochs=4, evaluation=True)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.592653   |     -      |     -     |   7.28   \n",
            "   1    |   40    |   0.453967   |     -      |     -     |   6.92   \n",
            "   1    |   60    |   0.384556   |     -      |     -     |   7.00   \n",
            "   1    |   80    |   0.357267   |     -      |     -     |   7.07   \n",
            "   1    |   100   |   0.362626   |     -      |     -     |   7.10   \n",
            "   1    |   120   |   0.353208   |     -      |     -     |   7.15   \n",
            "   1    |   140   |   0.391802   |     -      |     -     |   7.23   \n",
            "   1    |   160   |   0.360456   |     -      |     -     |   7.30   \n",
            "   1    |   180   |   0.334347   |     -      |     -     |   7.39   \n",
            "   1    |   200   |   0.354172   |     -      |     -     |   7.43   \n",
            "   1    |   220   |   0.348727   |     -      |     -     |   7.51   \n",
            "   1    |   240   |   0.324600   |     -      |     -     |   7.52   \n",
            "   1    |   260   |   0.348310   |     -      |     -     |   7.50   \n",
            "   1    |   280   |   0.381024   |     -      |     -     |   7.48   \n",
            "   1    |   300   |   0.363713   |     -      |     -     |   7.44   \n",
            "   1    |   320   |   0.342422   |     -      |     -     |   7.42   \n",
            "   1    |   340   |   0.348766   |     -      |     -     |   7.42   \n",
            "   1    |   360   |   0.363724   |     -      |     -     |   7.37   \n",
            "   1    |   380   |   0.316239   |     -      |     -     |   7.39   \n",
            "   1    |   400   |   0.320994   |     -      |     -     |   7.42   \n",
            "   1    |   420   |   0.313102   |     -      |     -     |   7.41   \n",
            "   1    |   440   |   0.367953   |     -      |     -     |   7.42   \n",
            "   1    |   460   |   0.302404   |     -      |     -     |   7.42   \n",
            "   1    |   480   |   0.292961   |     -      |     -     |   7.45   \n",
            "   1    |   500   |   0.346716   |     -      |     -     |   7.47   \n",
            "   1    |   520   |   0.322528   |     -      |     -     |   7.45   \n",
            "   1    |   540   |   0.311202   |     -      |     -     |   7.43   \n",
            "   1    |   560   |   0.289880   |     -      |     -     |   7.43   \n",
            "   1    |   579   |   0.304249   |     -      |     -     |   6.77   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.354103   |  0.288708  |   87.03   |  220.85  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.240332   |     -      |     -     |   7.77   \n",
            "   2    |   40    |   0.210232   |     -      |     -     |   7.42   \n",
            "   2    |   60    |   0.237821   |     -      |     -     |   7.41   \n",
            "   2    |   80    |   0.258186   |     -      |     -     |   7.41   \n",
            "   2    |   100   |   0.240292   |     -      |     -     |   7.40   \n",
            "   2    |   120   |   0.218580   |     -      |     -     |   7.41   \n",
            "   2    |   140   |   0.232283   |     -      |     -     |   7.41   \n",
            "   2    |   160   |   0.203315   |     -      |     -     |   7.40   \n",
            "   2    |   180   |   0.219852   |     -      |     -     |   7.39   \n",
            "   2    |   200   |   0.239233   |     -      |     -     |   7.44   \n",
            "   2    |   220   |   0.219300   |     -      |     -     |   7.43   \n",
            "   2    |   240   |   0.220621   |     -      |     -     |   7.41   \n",
            "   2    |   260   |   0.193574   |     -      |     -     |   7.42   \n",
            "   2    |   280   |   0.218828   |     -      |     -     |   7.42   \n",
            "   2    |   300   |   0.207316   |     -      |     -     |   7.43   \n",
            "   2    |   320   |   0.192486   |     -      |     -     |   7.43   \n",
            "   2    |   340   |   0.232526   |     -      |     -     |   7.41   \n",
            "   2    |   360   |   0.210745   |     -      |     -     |   7.43   \n",
            "   2    |   380   |   0.236681   |     -      |     -     |   7.47   \n",
            "   2    |   400   |   0.218440   |     -      |     -     |   7.45   \n",
            "   2    |   420   |   0.214897   |     -      |     -     |   7.42   \n",
            "   2    |   440   |   0.223467   |     -      |     -     |   7.40   \n",
            "   2    |   460   |   0.212712   |     -      |     -     |   7.41   \n",
            "   2    |   480   |   0.202515   |     -      |     -     |   7.43   \n",
            "   2    |   500   |   0.258737   |     -      |     -     |   7.41   \n",
            "   2    |   520   |   0.212426   |     -      |     -     |   7.39   \n",
            "   2    |   540   |   0.245442   |     -      |     -     |   7.43   \n",
            "   2    |   560   |   0.216684   |     -      |     -     |   7.42   \n",
            "   2    |   579   |   0.231045   |     -      |     -     |   6.76   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.223070   |  0.286175  |   89.09   |  223.08  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.104821   |     -      |     -     |   7.79   \n",
            "   3    |   40    |   0.114122   |     -      |     -     |   7.43   \n",
            "   3    |   60    |   0.121174   |     -      |     -     |   7.41   \n",
            "   3    |   80    |   0.148596   |     -      |     -     |   7.43   \n",
            "   3    |   100   |   0.136232   |     -      |     -     |   7.41   \n",
            "   3    |   120   |   0.123170   |     -      |     -     |   7.41   \n",
            "   3    |   140   |   0.081903   |     -      |     -     |   7.37   \n",
            "   3    |   160   |   0.118563   |     -      |     -     |   7.42   \n",
            "   3    |   180   |   0.121077   |     -      |     -     |   7.40   \n",
            "   3    |   200   |   0.128736   |     -      |     -     |   7.40   \n",
            "   3    |   220   |   0.096760   |     -      |     -     |   7.39   \n",
            "   3    |   240   |   0.126478   |     -      |     -     |   7.43   \n",
            "   3    |   260   |   0.119079   |     -      |     -     |   7.39   \n",
            "   3    |   280   |   0.141610   |     -      |     -     |   7.41   \n",
            "   3    |   300   |   0.157525   |     -      |     -     |   7.42   \n",
            "   3    |   320   |   0.116598   |     -      |     -     |   7.38   \n",
            "   3    |   340   |   0.091728   |     -      |     -     |   7.41   \n",
            "   3    |   360   |   0.109118   |     -      |     -     |   7.39   \n",
            "   3    |   380   |   0.112415   |     -      |     -     |   7.40   \n",
            "   3    |   400   |   0.108360   |     -      |     -     |   7.42   \n",
            "   3    |   420   |   0.143652   |     -      |     -     |   7.42   \n",
            "   3    |   440   |   0.099596   |     -      |     -     |   7.44   \n",
            "   3    |   460   |   0.129091   |     -      |     -     |   7.41   \n",
            "   3    |   480   |   0.130462   |     -      |     -     |   7.45   \n",
            "   3    |   500   |   0.130678   |     -      |     -     |   7.40   \n",
            "   3    |   520   |   0.148213   |     -      |     -     |   7.44   \n",
            "   3    |   540   |   0.090536   |     -      |     -     |   7.43   \n",
            "   3    |   560   |   0.129443   |     -      |     -     |   7.40   \n",
            "   3    |   579   |   0.219381   |     -      |     -     |   6.77   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.123910   |  0.344186  |   89.33   |  222.88  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.088036   |     -      |     -     |   7.73   \n",
            "   4    |   40    |   0.077385   |     -      |     -     |   7.42   \n",
            "   4    |   60    |   0.041158   |     -      |     -     |   7.38   \n",
            "   4    |   80    |   0.042458   |     -      |     -     |   7.39   \n",
            "   4    |   100   |   0.083414   |     -      |     -     |   7.40   \n",
            "   4    |   120   |   0.067083   |     -      |     -     |   7.39   \n",
            "   4    |   140   |   0.073280   |     -      |     -     |   7.38   \n",
            "   4    |   160   |   0.048763   |     -      |     -     |   7.38   \n",
            "   4    |   180   |   0.061229   |     -      |     -     |   7.38   \n",
            "   4    |   200   |   0.093645   |     -      |     -     |   7.37   \n",
            "   4    |   220   |   0.098651   |     -      |     -     |   7.37   \n",
            "   4    |   240   |   0.042530   |     -      |     -     |   7.37   \n",
            "   4    |   260   |   0.073387   |     -      |     -     |   7.39   \n",
            "   4    |   280   |   0.036092   |     -      |     -     |   7.37   \n",
            "   4    |   300   |   0.093674   |     -      |     -     |   7.41   \n",
            "   4    |   320   |   0.108257   |     -      |     -     |   7.41   \n",
            "   4    |   340   |   0.063628   |     -      |     -     |   7.41   \n",
            "   4    |   360   |   0.062631   |     -      |     -     |   7.38   \n",
            "   4    |   380   |   0.064999   |     -      |     -     |   7.41   \n",
            "   4    |   400   |   0.057033   |     -      |     -     |   7.40   \n",
            "   4    |   420   |   0.067840   |     -      |     -     |   7.43   \n",
            "   4    |   440   |   0.073193   |     -      |     -     |   7.42   \n",
            "   4    |   460   |   0.068212   |     -      |     -     |   7.39   \n",
            "   4    |   480   |   0.081680   |     -      |     -     |   7.39   \n",
            "   4    |   500   |   0.084996   |     -      |     -     |   7.39   \n",
            "   4    |   520   |   0.066740   |     -      |     -     |   7.38   \n",
            "   4    |   540   |   0.051454   |     -      |     -     |   7.38   \n",
            "   4    |   560   |   0.057022   |     -      |     -     |   7.39   \n",
            "   4    |   579   |   0.048121   |     -      |     -     |   6.72   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.068227   |  0.428724  |   89.23   |  222.21  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            ">>>>>Training complete! in  0:14:49.016003  secondes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFzFKvdFB8dH"
      },
      "source": [
        "## 3.4. Evaluation on Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX6lTCjeB-ph"
      },
      "source": [
        "The prediction step is similar to the evaluation step that we did in the training loop, but simpler. We will perform a forward pass to compute logits and apply softmax to calculate probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNSzs6ZfCBDH"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Tell PyTorch to run the model on GPU or cpu\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # Load batch to GPU or cpu\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-TsZo5NunE1"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "    \n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "tk680d_-CFnD",
        "outputId": "01db7466-3ae7-4357-88fd-a13199a7707d"
      },
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_discri_dataloader)\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_discri_val)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.9301\n",
            "Accuracy: 89.12%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfhm86fpCTgb"
      },
      "source": [
        "### **3.5. Train Our Model on the Entire Training Data**¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EZh1dB0CYg1",
        "outputId": "9aa7c7e1-58a3-41ec-b80f-6fcf19b75c26"
      },
      "source": [
        "# Concatenate the train set and the validation set\n",
        "full_train_data = torch.utils.data.ConcatDataset([train_discri_data, val_discri_data])\n",
        "full_train_sampler = RandomSampler(full_train_data)\n",
        "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n",
        "\n",
        "# Train the Bert Classifier on the entire training data\n",
        "set_seed(42)\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=4)\n",
        "train(bert_classifier, full_train_dataloader, epochs=4)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.554313   |     -      |     -     |   7.85   \n",
            "   1    |   40    |   0.394482   |     -      |     -     |   7.67   \n",
            "   1    |   60    |   0.415003   |     -      |     -     |   7.81   \n",
            "   1    |   80    |   0.431460   |     -      |     -     |   7.75   \n",
            "   1    |   100   |   0.407126   |     -      |     -     |   7.61   \n",
            "   1    |   120   |   0.348311   |     -      |     -     |   7.44   \n",
            "   1    |   140   |   0.350590   |     -      |     -     |   7.34   \n",
            "   1    |   160   |   0.292606   |     -      |     -     |   7.30   \n",
            "   1    |   180   |   0.387515   |     -      |     -     |   7.29   \n",
            "   1    |   200   |   0.368568   |     -      |     -     |   7.33   \n",
            "   1    |   220   |   0.360253   |     -      |     -     |   7.36   \n",
            "   1    |   240   |   0.319501   |     -      |     -     |   7.44   \n",
            "   1    |   260   |   0.341014   |     -      |     -     |   7.51   \n",
            "   1    |   280   |   0.343883   |     -      |     -     |   7.55   \n",
            "   1    |   300   |   0.333573   |     -      |     -     |   7.54   \n",
            "   1    |   320   |   0.307193   |     -      |     -     |   7.49   \n",
            "   1    |   340   |   0.294982   |     -      |     -     |   7.51   \n",
            "   1    |   360   |   0.330162   |     -      |     -     |   7.41   \n",
            "   1    |   380   |   0.321841   |     -      |     -     |   7.41   \n",
            "   1    |   400   |   0.364924   |     -      |     -     |   7.40   \n",
            "   1    |   420   |   0.294846   |     -      |     -     |   7.40   \n",
            "   1    |   440   |   0.391254   |     -      |     -     |   7.40   \n",
            "   1    |   460   |   0.296263   |     -      |     -     |   7.41   \n",
            "   1    |   480   |   0.330921   |     -      |     -     |   7.42   \n",
            "   1    |   500   |   0.290938   |     -      |     -     |   7.42   \n",
            "   1    |   520   |   0.320433   |     -      |     -     |   7.46   \n",
            "   1    |   540   |   0.311325   |     -      |     -     |   7.46   \n",
            "   1    |   560   |   0.302509   |     -      |     -     |   7.47   \n",
            "   1    |   580   |   0.380253   |     -      |     -     |   7.47   \n",
            "   1    |   600   |   0.297877   |     -      |     -     |   7.44   \n",
            "   1    |   620   |   0.285924   |     -      |     -     |   7.46   \n",
            "   1    |   640   |   0.325057   |     -      |     -     |   7.47   \n",
            "   1    |   643   |   0.337212   |     -      |     -     |   0.94   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.207110   |     -      |     -     |   7.82   \n",
            "   2    |   40    |   0.201173   |     -      |     -     |   7.47   \n",
            "   2    |   60    |   0.269544   |     -      |     -     |   7.46   \n",
            "   2    |   80    |   0.207008   |     -      |     -     |   7.43   \n",
            "   2    |   100   |   0.241385   |     -      |     -     |   7.47   \n",
            "   2    |   120   |   0.233836   |     -      |     -     |   7.43   \n",
            "   2    |   140   |   0.195373   |     -      |     -     |   7.42   \n",
            "   2    |   160   |   0.210929   |     -      |     -     |   7.43   \n",
            "   2    |   180   |   0.235914   |     -      |     -     |   7.43   \n",
            "   2    |   200   |   0.233798   |     -      |     -     |   7.42   \n",
            "   2    |   220   |   0.203382   |     -      |     -     |   7.45   \n",
            "   2    |   240   |   0.177664   |     -      |     -     |   7.41   \n",
            "   2    |   260   |   0.201497   |     -      |     -     |   7.44   \n",
            "   2    |   280   |   0.244598   |     -      |     -     |   7.42   \n",
            "   2    |   300   |   0.241540   |     -      |     -     |   7.43   \n",
            "   2    |   320   |   0.218316   |     -      |     -     |   7.45   \n",
            "   2    |   340   |   0.194770   |     -      |     -     |   7.43   \n",
            "   2    |   360   |   0.232002   |     -      |     -     |   7.40   \n",
            "   2    |   380   |   0.212375   |     -      |     -     |   7.42   \n",
            "   2    |   400   |   0.216499   |     -      |     -     |   7.43   \n",
            "   2    |   420   |   0.204701   |     -      |     -     |   7.43   \n",
            "   2    |   440   |   0.207856   |     -      |     -     |   7.41   \n",
            "   2    |   460   |   0.215917   |     -      |     -     |   7.44   \n",
            "   2    |   480   |   0.217419   |     -      |     -     |   7.43   \n",
            "   2    |   500   |   0.208826   |     -      |     -     |   7.42   \n",
            "   2    |   520   |   0.190772   |     -      |     -     |   7.42   \n",
            "   2    |   540   |   0.220126   |     -      |     -     |   7.44   \n",
            "   2    |   560   |   0.193929   |     -      |     -     |   7.45   \n",
            "   2    |   580   |   0.198144   |     -      |     -     |   7.46   \n",
            "   2    |   600   |   0.210213   |     -      |     -     |   7.45   \n",
            "   2    |   620   |   0.195466   |     -      |     -     |   7.46   \n",
            "   2    |   640   |   0.241641   |     -      |     -     |   7.46   \n",
            "   2    |   643   |   0.177527   |     -      |     -     |   0.93   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.096449   |     -      |     -     |   7.81   \n",
            "   3    |   40    |   0.119931   |     -      |     -     |   7.42   \n",
            "   3    |   60    |   0.069052   |     -      |     -     |   7.43   \n",
            "   3    |   80    |   0.082781   |     -      |     -     |   7.44   \n",
            "   3    |   100   |   0.122408   |     -      |     -     |   7.47   \n",
            "   3    |   120   |   0.130170   |     -      |     -     |   7.43   \n",
            "   3    |   140   |   0.140401   |     -      |     -     |   7.41   \n",
            "   3    |   160   |   0.115596   |     -      |     -     |   7.42   \n",
            "   3    |   180   |   0.103440   |     -      |     -     |   7.41   \n",
            "   3    |   200   |   0.131483   |     -      |     -     |   7.42   \n",
            "   3    |   220   |   0.089769   |     -      |     -     |   7.41   \n",
            "   3    |   240   |   0.124613   |     -      |     -     |   7.43   \n",
            "   3    |   260   |   0.154980   |     -      |     -     |   7.42   \n",
            "   3    |   280   |   0.141424   |     -      |     -     |   7.45   \n",
            "   3    |   300   |   0.118272   |     -      |     -     |   7.45   \n",
            "   3    |   320   |   0.118203   |     -      |     -     |   7.42   \n",
            "   3    |   340   |   0.129573   |     -      |     -     |   7.44   \n",
            "   3    |   360   |   0.114923   |     -      |     -     |   7.42   \n",
            "   3    |   380   |   0.114204   |     -      |     -     |   7.47   \n",
            "   3    |   400   |   0.109631   |     -      |     -     |   7.42   \n",
            "   3    |   420   |   0.101082   |     -      |     -     |   7.40   \n",
            "   3    |   440   |   0.134344   |     -      |     -     |   7.43   \n",
            "   3    |   460   |   0.137336   |     -      |     -     |   7.43   \n",
            "   3    |   480   |   0.126673   |     -      |     -     |   7.41   \n",
            "   3    |   500   |   0.087641   |     -      |     -     |   7.42   \n",
            "   3    |   520   |   0.103177   |     -      |     -     |   7.42   \n",
            "   3    |   540   |   0.113231   |     -      |     -     |   7.43   \n",
            "   3    |   560   |   0.089812   |     -      |     -     |   7.43   \n",
            "   3    |   580   |   0.092143   |     -      |     -     |   7.41   \n",
            "   3    |   600   |   0.099441   |     -      |     -     |   7.43   \n",
            "   3    |   620   |   0.085253   |     -      |     -     |   7.43   \n",
            "   3    |   640   |   0.111979   |     -      |     -     |   7.44   \n",
            "   3    |   643   |   0.112861   |     -      |     -     |   0.95   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.060280   |     -      |     -     |   7.78   \n",
            "   4    |   40    |   0.060578   |     -      |     -     |   7.42   \n",
            "   4    |   60    |   0.086815   |     -      |     -     |   7.45   \n",
            "   4    |   80    |   0.043438   |     -      |     -     |   7.36   \n",
            "   4    |   100   |   0.053053   |     -      |     -     |   7.42   \n",
            "   4    |   120   |   0.073636   |     -      |     -     |   7.43   \n",
            "   4    |   140   |   0.062491   |     -      |     -     |   7.41   \n",
            "   4    |   160   |   0.091351   |     -      |     -     |   7.42   \n",
            "   4    |   180   |   0.047643   |     -      |     -     |   7.37   \n",
            "   4    |   200   |   0.056489   |     -      |     -     |   7.42   \n",
            "   4    |   220   |   0.041675   |     -      |     -     |   7.39   \n",
            "   4    |   240   |   0.061426   |     -      |     -     |   7.41   \n",
            "   4    |   260   |   0.069762   |     -      |     -     |   7.43   \n",
            "   4    |   280   |   0.054286   |     -      |     -     |   7.41   \n",
            "   4    |   300   |   0.050052   |     -      |     -     |   7.43   \n",
            "   4    |   320   |   0.070303   |     -      |     -     |   7.44   \n",
            "   4    |   340   |   0.046516   |     -      |     -     |   7.42   \n",
            "   4    |   360   |   0.058370   |     -      |     -     |   7.43   \n",
            "   4    |   380   |   0.051581   |     -      |     -     |   7.44   \n",
            "   4    |   400   |   0.060483   |     -      |     -     |   7.42   \n",
            "   4    |   420   |   0.060443   |     -      |     -     |   7.40   \n",
            "   4    |   440   |   0.041740   |     -      |     -     |   7.39   \n",
            "   4    |   460   |   0.060549   |     -      |     -     |   7.42   \n",
            "   4    |   480   |   0.056455   |     -      |     -     |   7.41   \n",
            "   4    |   500   |   0.065971   |     -      |     -     |   7.40   \n",
            "   4    |   520   |   0.090880   |     -      |     -     |   7.41   \n",
            "   4    |   540   |   0.055662   |     -      |     -     |   7.40   \n",
            "   4    |   560   |   0.050144   |     -      |     -     |   7.43   \n",
            "   4    |   580   |   0.094309   |     -      |     -     |   7.41   \n",
            "   4    |   600   |   0.061531   |     -      |     -     |   7.43   \n",
            "   4    |   620   |   0.074814   |     -      |     -     |   7.42   \n",
            "   4    |   640   |   0.040776   |     -      |     -     |   7.41   \n",
            "   4    |   643   |   0.005717   |     -      |     -     |   0.92   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            ">>>>>Training complete! in  0:15:57.065978  secondes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26loU_tZDo2c"
      },
      "source": [
        "### **4. Predictions on Test Set**¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdjSjpAJDu8B",
        "outputId": "a06bc5a6-7184-452a-82eb-1742ff1ea5e4"
      },
      "source": [
        "# Run `preprocessing_for_bert` on the test set\n",
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(data_test.Tweets)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>>>> La Tokenization à pris : 0:00:01.001654  secondes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrPnHMfMD4SG"
      },
      "source": [
        "### **4.2. Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77rOC6qeD_Zf",
        "outputId": "2e877a57-12da-4f36-9d1f-c4fb94624348"
      },
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.9\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"Number of tweets bad predicted: \", preds.sum())"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tweets bad predicted:  562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib-_WUoqz-Is"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}